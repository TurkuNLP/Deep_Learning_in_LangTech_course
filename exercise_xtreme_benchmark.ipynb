{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOo8HgvOI9U9Sd7Qi7jBiOK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/exercise_xtreme_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBCWIyQx7Cb_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XTREME:  Benchmark for Zero-Shot Cross-Lingual Transfer\n",
        "\n",
        "### a) What is zero-shot cross-lingual transfer?\n",
        "\n",
        "Zero-shot cross-lingual transfer refers to model's ability **to perform tasks in languages it wasn't specifically trained on**. It involves transferring knowledge learned from one language to another without direct training on the target language.\n",
        "\n",
        "This approach is especially useful for languages with limited data resources.\n",
        "\n",
        "### b) What transfer approaches they test in the paper?\n",
        "\n",
        "* **Model transfer:** pretrained multilingual representations (mBERT, XLM-R etc.) are fine-tuned on English labelled data. The model is then evaluated on the test data of the task in the target languages.\n",
        "\n",
        "Baselines:\n",
        "\n",
        "* **Translate train:** Machine translate the English training data into the target language, fine-tune mBERT on the target language data\n",
        "* **Translate test:**  Train the English BERT-Large model on the English training\n",
        "data and evaluate it on test data machine translated from the target language to English\n",
        "* **In-language:** if target-language training data is available, fine-tune mBERT on target language data\n",
        "* **In-language few-shot:** fine-tune mBERT on 1,000 target language examples\n",
        "\n",
        "### c) What kind of datasets/tasks are used in evaluation? What is the general evaluation setting (e.g. train/test languages)?\n",
        "\n",
        "Tasks:\n",
        "\n",
        "* **Sentence pair classification**\n",
        "  - XNLI: _A soccer game with multiple males playing._ ---\t_Some men are playing a sport._ Entail, Contradict, or Neutral?\n",
        "  - PAWS-X: _Mark Williams is played in the TV series by Olaf Petersen._ --- _Olaf Petersen is played by Mark Williams in the television series._ Paraphrase or not?\n",
        "* **Structured prediction**\n",
        "  - POS: I/PRON ran/VERB across/ADP this/DET item/NOUN on/ADP the/DET Internet/ NOUN ./PUNCT\n",
        "  - NER: Jim/PERSON bought 300 shares of Acme/ORG Corp/ORG in 2006/TIME .\n",
        "* **Question Answering** (extractive, given a question and a context document, find the answer span)\n",
        "  - XQuAD\n",
        "  - MLQA\n",
        "  - TyDiQA\n",
        "* **Sentence retrieval**\n",
        "  - BUCC: recognize translations from a comparable corpus\n",
        "  - Tatoeba: recognize translations\n",
        "\n",
        "\n",
        "**Language**\n",
        "* Train: English\n",
        "* Evaluate: total 40 languages (not all datasets available for all languages)\n",
        "\n",
        "### d) Summarize their overall findings. How well zero-shot cross-lingual transfer seems to work in their setting, and how do different methods compere with each other?\n",
        "\n",
        "* Table 2 in the paper\n",
        "* XLM-R is the best-performing zero-shot transfer model\n",
        "* If a strong MT system is available, translating the training\n",
        "sets provides improvements over model transfer\n",
        "* Translating the test data provides similar results compared to translating the training data (more expensive during inference time)\n",
        "* 1,000 in-language training examples outperforms model transfer on POS and NER, but not in QA\n",
        "* **These results are from 2020, a lot have happened since!**"
      ],
      "metadata": {
        "id": "ccdshUvF7VTe"
      }
    }
  ]
}