{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/hf_trainer_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "First, set up the required Python modules and perform some general configuration.\n",
        "\n",
        "(This part of the code follows the [CNN notebook](https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/hf_trainer_cnn.ipynb) that you should already be familiar with.)"
      ],
      "metadata": {
        "id": "wo13ZXoZYB6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the required Python packages using [pip](https://en.wikipedia.org/wiki/Pip):\n",
        "\n",
        "* [`transformers`](https://huggingface.co/docs/transformers/index) is a popular deep learning package\n",
        "* [`datasets`](https://huggingface.co/docs/datasets/) provides support for loading, creating, and manipulating datasets\n",
        "* [`evaluate`](https://huggingface.co/docs/evaluate/index) is a library for easily evaluating machine learning models and datasets\n",
        "* [`accelerate`](https://pypi.org/project/accelerate/) is a wrapper we need to install in order to train torch models using a transformers trainer\n",
        "\n",
        "Both `transformers` and `datasets` are used extensively on this course."
      ],
      "metadata": {
        "id": "qKDDc9isYnxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q transformers datasets evaluate accelerate"
      ],
      "metadata": {
        "id": "4pquj9Xoxaza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Above, the `!` at the start of the line tells the notebook to run the line as an operating system command rather than Python code, and the `-q` argument to `pip` runs the command in \"quiet\" mode, with less output.)"
      ],
      "metadata": {
        "id": "JyyZCbB0Yxmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also use the [`pprint`](https://docs.python.org/3/library/pprint.html) (\"pretty-print\") module to format output more readably below. The only difference to just using `print` is that some data structures will be easier to read and interpret."
      ],
      "metadata": {
        "id": "N2oLE1FpY09U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import PrettyPrinter\n",
        "\n",
        "pprint = PrettyPrinter(compact=True).pprint"
      ],
      "metadata": {
        "id": "cQ63zw6BY7tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will reduce logging output. The `transformers` library by default produces fairly verbose logging. Commenting out the following code will enable low-priority output (`INFO` logging level and below)."
      ],
      "metadata": {
        "id": "xkrZ6e_FY_2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.disable(logging.INFO)"
      ],
      "metadata": {
        "id": "ZH2bY_GjZG24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Download and prepare data\n",
        "\n",
        "We will again use the `datasets` library function [`load_dataset`](https://huggingface.co/docs/datasets/master/en/package_reference/loading_methods#datasets.load_dataset) to load a dataset for our experiments."
      ],
      "metadata": {
        "id": "W4x7GbT2ZKUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "\n",
        "dataset = datasets.load_dataset(\"imdb\")"
      ],
      "metadata": {
        "id": "5DKskTuoyCf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what the dataset contains:"
      ],
      "metadata": {
        "id": "FVo6_CS2eeIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "y3K4Fkz7yce7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and print out an example:"
      ],
      "metadata": {
        "id": "o17ZWi_kf4AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle()\n",
        "del dataset[\"unsupervised\"]\n",
        "pprint(dataset[\"train\"][0])"
      ],
      "metadata": {
        "id": "Wd__TIUnf9PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Tokenize and vectorize data\n",
        "\n",
        "(This part of the code follows the [CNN notebook](https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/hf_trainer_cnn.ipynb) that you should already be familiar with.)"
      ],
      "metadata": {
        "id": "8KF9UtzUbrBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tokenize and vectorize the texts of our dataset, we will again use previously created tokenizers through the simple [`AutoTokenizer`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer) class.\n",
        "\n",
        "The [`AutoTokenizer.from_pretrained`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer.from_pretrained) function can load the tokenizer associated with any of the large number of models found in the [Hugging Face models repository](https://huggingface.co/models). Here, our texts are in English, and we'll load the tokenizer for the [`bert-base-cased`](https://huggingface.co/bert-base-cased) model.\n",
        "\n",
        "(**Note**: we're not actually using the BERT model here, just its tokenizer.)"
      ],
      "metadata": {
        "id": "H9RBff6Rkfyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "model_name = \"bert-base-cased\"\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "wjrAGcFtymJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in the [CNN notebook](https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/hf_trainer_cnn.ipynb), we will define a simple tokenization function and tokenize and vectorize our whole dataset with the tokenizer by calling the [`Dataset.map`](https://huggingface.co/docs/datasets/v2.14.4/en/package_reference/main_classes#datasets.Dataset.map) function.\n",
        "\n",
        "Note that here we're providing a `max_length` argument and `truncation=True` in the tokenizer call. This limits the maximum length of outputs to the given length (see the [tokenizers documentation](https://huggingface.co/docs/transformers/preprocessing#everything-you-always-wanted-to-know-about-padding-and-truncation) for details). This makes training faster, potentially at some cost in performance."
      ],
      "metadata": {
        "id": "mjEeyPsXc12X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple function that applies the tokenizer\n",
        "def tokenize(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "# Apply the tokenizer to the whole dataset using .map()\n",
        "dataset = dataset.map(tokenize)"
      ],
      "metadata": {
        "id": "K2c4lFKTywlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Build model\n",
        "\n",
        "As usual, we will create a PyTorch model class with an `__init__()` function that creates the layers and a `forward()` function which implements the actual computation. For more information on these, please see the [PyTorch tutorial](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html)."
      ],
      "metadata": {
        "id": "AiX7WeW2X5gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're here building a simple RNN with the following structure:\n",
        "\n",
        "* As in the [CNN](https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/hf_trainer_cnn.ipynb), the token IDs are first mapped to embeddings of a user-specified size (`config.embedding_dim`) in a [torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) layer. Note that here the embeddings are initialized randomly and learned along with other model weights. In real-world applications, the embeddings would typically be initialized with previously learned weights.\n",
        "* Second, the embedded inputs are passed through an RNN ([torch.nn.RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)), which produces a series of outputs ($(y_1, \\ldots, y_n)$, where $n$ is the length of the input) and the final hidden state $h_n$. Here, we will only use the last output $y_n$.\n",
        "* Finally, there is a fully connected layer ([torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) that maps the last RNN output to the two possible output values of the classifier.\n",
        "\n",
        "We can interpret this model as processing the input step by step, attempting to identify tokens (embeddings) that in the context of its previous input express either positive or negative opinions, and to output a value at the end of the sequence that can be mapped to the positive or negative class."
      ],
      "metadata": {
        "id": "8UQlKM1Vfrha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the `forward` function we mostly just pass the input through the layers, with the following additional steps:\n",
        "\n",
        "* To invoke the RNN, we need to provide the value of the initial hidden state $h_0$. Here we simply use [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html) to create a tensor of the appropriate size filled with zeros.\n",
        "* To get the value of the last item in the sequence of RNN outputs (`rnn_outputs`), we slice the three-dimensional tensor (batch, rnn step, output dim) with `rnn_outputs[:, -1, :]`. This returns all values in the first and last dimensions, and the last in the second. If you are not familar with this syntax, consider the following example:"
      ],
      "metadata": {
        "id": "mZqIlxnDOxbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "\n",
        "a = numpy.array([\n",
        "  [[11], [12], [13]],\n",
        "  [[21], [22], [23]],\n",
        "  [[31], [32], [33]],\n",
        "])\n",
        "\n",
        "print(a[:, -1, :])"
      ],
      "metadata": {
        "id": "IPWGCx8ODwS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(The two points above can be considered technical details and understanding them in detail is not required to understand the model.)"
      ],
      "metadata": {
        "id": "JlGCWXiSEvH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the model:"
      ],
      "metadata": {
        "id": "UCgAqPAeEypM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# This gives a new name to the config class, just for convenience\n",
        "BasicConfig = transformers.PretrainedConfig\n",
        "\n",
        "\n",
        "# This is the model\n",
        "class SimpleRNN(transformers.PreTrainedModel):\n",
        "\n",
        "    config_class = BasicConfig\n",
        "\n",
        "    # In the initialization method, one instantiates the layers\n",
        "    # these will be the parameters of the model\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        # Embedding layer: vocab size x embedding dim\n",
        "        self.embeddings = torch.nn.Embedding(\n",
        "            num_embeddings=config.vocab_size,\n",
        "            embedding_dim=config.embedding_dim\n",
        "        )\n",
        "        # RNN with configurable hidden size and nonlinearity\n",
        "        self.rnn = torch.nn.RNN(\n",
        "            input_size=config.embedding_dim,\n",
        "            hidden_size=config.hidden_size,\n",
        "            num_layers=config.num_layers,\n",
        "            nonlinearity=config.nonlinearity,\n",
        "            batch_first=True\n",
        "        )\n",
        "        # Output layer: embedding size to output size\n",
        "        self.output_layer = torch.nn.Linear(\n",
        "            in_features=config.hidden_size,\n",
        "            out_features=config.num_labels\n",
        "        )\n",
        "        # Loss function: standard loss for classification\n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
        "        # Embed input ids\n",
        "        x = self.embeddings(input_ids)\n",
        "        # Set initial hidden state to all-zero values\n",
        "        batch_size = x.shape[0]\n",
        "        h0 = torch.zeros(\n",
        "            (self.config.num_layers, batch_size, self.config.hidden_size),\n",
        "            device=input_ids.device    # place on same device as input\n",
        "        )\n",
        "        # Run RNN repeatedly to get sequence of outputs and last hidden state\n",
        "        rnn_outputs, h_n = self.rnn(x, h0)\n",
        "        # Get last RNN output\n",
        "        y_n = rnn_outputs[:, -1, :]\n",
        "        # Map to outputs with fully connected layer\n",
        "        output = self.output_layer(y_n)\n",
        "\n",
        "        # Return value computed as in CNN:\n",
        "        if labels is not None:\n",
        "            # We have labels, so we can calculate the loss\n",
        "            return (self.loss(output,labels), output)\n",
        "        else:\n",
        "            # No labels, so just return the output\n",
        "            return (output,)"
      ],
      "metadata": {
        "id": "9kyQQhu0_wep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Configure and train model"
      ],
      "metadata": {
        "id": "Tmj91CdCT-fJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll first configure and instantiate the model. Here `vocab_size` should always be the vocabulary size of the tokenizer and `num_labels` the number of unique labels in the data (as here), but the others are hyperparameters that you can choose:\n",
        "\n",
        "* `embedding_dim`: the size of the word (i.e. token) embeddings\n",
        "* `hidden_size`: the size of the RNN hidden state vector _h_\n",
        "* `num_layers`: number of stacked RNN layers\n",
        "* `nonlinearity`: the non-linear function to apply in RNN (`'tanh'` or `'relu'`)"
      ],
      "metadata": {
        "id": "pVTopuNPW26S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = BasicConfig(\n",
        "    vocab_size = tokenizer.vocab_size,\n",
        "    num_labels = len(set(dataset[\"train\"][\"label\"])),\n",
        "    embedding_dim = 64,\n",
        "    hidden_size = 96,\n",
        "    num_layers = 1,\n",
        "    nonlinearity = \"tanh\",\n",
        ")\n",
        "\n",
        "model = SimpleRNN(config)"
      ],
      "metadata": {
        "id": "nd_usta7WzTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training arguments are set similarly as in the [CNN notebook](https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/hf_trainer_cnn.ipynb). Many number of these settings relate to the frequency of evaluation and output during training, but the following are hyperparameters that you may wish to adjust:\n",
        "\n",
        "* `learning_rate`: the step size for weight updates\n",
        "* `per_device_train_batch_size`: number of examples per batch\n",
        "* `max_steps`: the maximum number of steps to train for"
      ],
      "metadata": {
        "id": "q5xkBXWDUL-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training arguments\n",
        "trainer_args = transformers.TrainingArguments(\n",
        "    \"checkpoints\",\n",
        "    eval_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    eval_steps=500,\n",
        "    logging_steps=500,\n",
        "    learning_rate=0.001,\n",
        "    per_device_train_batch_size=8,\n",
        "    max_steps=2500,\n",
        "    report_to=\"none\",\n",
        ")"
      ],
      "metadata": {
        "id": "gVW-yGhYItWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll then define the standard accuracy metric (ratio of correct out of all predictions), create a [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorWithPadding) to pad inputs to the same length (as required for batching) and an [EarlyStoppingCallback](https://huggingface.co/docs/transformers/main_classes/callback#transformers.EarlyStoppingCallback) to stop training when performance fails to improve for the given number of evaluations.\n",
        "\n",
        "(These should all be familiar to you from the [CNN notebook](https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/hf_trainer_cnn.ipynb))"
      ],
      "metadata": {
        "id": "lRE6YtHvUTcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "def compute_accuracy(outputs_and_labels):\n",
        "    outputs, labels = outputs_and_labels\n",
        "    predictions = outputs.argmax(axis=-1) #pick the index of the \"winning\" label\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "data_collator = transformers.DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "# Argument gives the number of steps of patience before early stopping\n",
        "early_stopping = transformers.EarlyStoppingCallback(\n",
        "    early_stopping_patience=5\n",
        ")"
      ],
      "metadata": {
        "id": "miilVFfCIhR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, as in the [CNN notebook](https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/hf_trainer_cnn.ipynb), we'll create a simple custom [callback](https://huggingface.co/docs/transformers/main_classes/callback) to store values logged during training so that we can more easily examine them later. (This is only needed for visualization and is not necessary to understand in detail.)"
      ],
      "metadata": {
        "id": "pd3o8RIdaWqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class LogSavingCallback(transformers.TrainerCallback):\n",
        "    def on_train_begin(self, *args, **kwargs):\n",
        "        self.logs = defaultdict(list)\n",
        "        self.training = True\n",
        "\n",
        "    def on_train_end(self, *args, **kwargs):\n",
        "        self.training = False\n",
        "\n",
        "    def on_log(self, args, state, control, logs, model=None, **kwargs):\n",
        "        if self.training:\n",
        "            for k, v in logs.items():\n",
        "                if k != \"epoch\" or v not in self.logs[k]:\n",
        "                    self.logs[k].append(v)\n",
        "\n",
        "training_logs = LogSavingCallback()"
      ],
      "metadata": {
        "id": "0EFQebkDYXES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then pass the model, trainer arguments, training and evaluation data, metric, the collator, and the callbacks to a [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) and call `.train()` to train the model."
      ],
      "metadata": {
        "id": "GCuvqB0bVPI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    args=trainer_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    compute_metrics=compute_accuracy,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[early_stopping, training_logs]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "4lKiSRNJVOvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Results"
      ],
      "metadata": {
        "id": "VId9dMvAO6aA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate and print out results:"
      ],
      "metadata": {
        "id": "h0VrtFCrO-LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate(dataset[\"test\"])\n",
        "\n",
        "pprint(eval_results)\n",
        "\n",
        "print('Accuracy:', eval_results['eval_accuracy'])"
      ],
      "metadata": {
        "id": "7zw3iJ7tPS7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also have a look at training and evaluation loss and evaluation accuracy progression as we did in the [CNN notebook](https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/hf_trainer_cnn.ipynb). (The code here is only for visualization and you do not need to understand it, but you should aim to be able to interpret the plots.)"
      ],
      "metadata": {
        "id": "xm1ZAFTObyf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot(logs, keys, labels):\n",
        "    values = sum([logs[k] for k in keys], [])\n",
        "    plt.ylim(max(min(values)-0.1, 0.0), min(max(values)+0.1, 1.0))\n",
        "    for key, label in zip(keys, labels):\n",
        "        plt.plot(logs[\"epoch\"], logs[key], label=label)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot(training_logs.logs, [\"loss\", \"eval_loss\"], [\"Training loss\", \"Evaluation loss\"])"
      ],
      "metadata": {
        "id": "4C2fPl02U9kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(training_logs.logs, [\"eval_accuracy\"], [\"Evaluation accuracy\"])"
      ],
      "metadata": {
        "id": "7cfERwdudGXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has barely learned anything. What's going on? Can you improve on the performance e.g. by modifying the hyperparameters?"
      ],
      "metadata": {
        "id": "X74MuyuIjnLS"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wo13ZXoZYB6J",
        "8KF9UtzUbrBA"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}