# Deep Learning in Human Language Technology course

Materials for the University of Turku course *TKO_8965 Deep Learning in Human Language Technology* (previously named *TKO_2101 Natural Language Processing*).

[UTU Moodle page](https://moodle.utu.fi/course/view.php?id=18315)


# Feed-forward NNs and the BoW model

Bag of words text classification with neural networks. On the lectures, we work our way through basic neural network models, their training, and application to classification.

* [Bag-of-words text classification - notebook](bow_classifier.ipynb)
* [Classifier word vector analysis - notebook](bow_classifier_features.ipynb)
* [Word embeddings - notebook](word_embeddings.ipynb)
* [BoW classifier with pretrained word embeddings - notebook](bow_classifier_embeddings_simpler.ipynb)

# Convolutional Neural Networks

Convolutional neural networks and their use in natural language processing.

* [Convolutional neural networks - slides](slides/Convolutional_neural_networks.pptx)
* [Sequence to Label with CNNs - notebook](seq2label_conv.ipynb)
* [CNN filter interpretation - notebook](cnn_filters.ipynb)
* [Pytorch CNN model - notebook](cnn_model_pytorch.ipynb)

# Recurrent Neural Networks

Introduction to recurrent neural networks and applications to various NLP tasks.

* [Recurrent neural networks - slides](slides/Recurrent_neural_networks.pdf)
* [Long short-term memory - slides](slides/Long_short-term_memory.pdf)
* [Text classification with recurrent neural networks - notebook](rnn_text_classification.ipynb)
* [Text generation with recurrent neural networks - notebook](rnn_text_generation.ipynb)
* [Named entity recognition with recurrent neural networks - notebook](rnn_named_entity_recognition.ipynb)

# Sequence to sequence and attention

Encoder-decorer and sequence-to-sequence architectures and introduction to neural attention.

* [Sequence to sequence and neural attention - slides](slides/Sequence_to_sequence_and_attention.pdf)
* [Sequence to sequence date normalization - notebook](seq2seq_dates.ipynb)
* [Sequence to sequence English to katakana translation - notebook](seq2seq_katakana.ipynb)
* [Neural machine translation with attention - TensorFlow tutorial](https://www.tensorflow.org/tutorials/text/nmt_with_attention)

# Transformer and transfer learning

Self-attention, transformer model, and deep transfer learning.

* [Transformer and transfer learning - slides](slides/Transformer_and_transfer_learning.pdf)
* [Deep neural language models - slides](slides/Deep_neural_language_models.pdf)
* [Text classification with BERT - notebook](bert_text_classification.ipynb)
* [Sequence labeling with BERT - notebook](bert_sequence_labeling.ipynb)

# Applications and evaluation

NLP applications of neural networks and evaluation of NN models.

* [Sequence to sequence applications - notebook](seq2seq_applications.ipynb)
* [Crosslingual sentence representations - slides](slides/Crosslingual_sentence_representations.pdf)
* [Laser and BERT embeddings - notebook](laser.ipynb)
* [Inference as benchmark - slides](slides/Inference_as_benchmark.pdf)
* [Additional information on evaluation and paraphrase datasets](Paraphrase_corpora.pdf)
