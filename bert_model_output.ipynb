{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_model_output.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNeAail3UKDMbLM1DHPxodT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/bert_model_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic BERT operations\n"
      ],
      "metadata": {
        "id": "U2k3QEnj0hTN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuY096pr0DK_"
      },
      "outputs": [],
      "source": [
        "!pip3 -q install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import datasets\n",
        "import torch"
      ],
      "metadata": {
        "id": "WMej7EPhAFRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\") #you can also use the trusty \"TurkuNLP/bert-base-finnish-cased-v1\""
      ],
      "metadata": {
        "id": "SItSFN_-0sTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will be running the model directly, so let's use return_tensors=\"pt\" to get torch tensors rather than Python lists\n",
        "texts=[\"Dogs like to [MASK] cats. They taste good.\",\"Bad joke!\"]\n",
        "t=tokenizer(texts,padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(\"Input ids\",t[\"input_ids\"])\n",
        "print(\"Token type ids\",t[\"token_type_ids\"])\n",
        "print(\"Attention mask\",t[\"attention_mask\"])"
      ],
      "metadata": {
        "id": "G3gKwiAf1Fg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is what the first sequence looks like\n",
        "tokenizer.decode(t[\"input_ids\"][0])"
      ],
      "metadata": {
        "id": "Hed6oOCO1Z1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT: bare model\n",
        "* How to use the bare model\n",
        "* What does it give us?"
      ],
      "metadata": {
        "id": "tGout3eyB0qE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert=transformers.AutoModel.from_pretrained(\"bert-base-cased\") #\"TurkuNLP/bert-base-finnish-cased-v1\" if you run this in Finnish\n"
      ],
      "metadata": {
        "id": "dF6eNBzO16Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* in torch the model's forward() function tends to be mapped to `__call__()` i.e. it is used when you call the model as if it were a function\n"
      ],
      "metadata": {
        "id": "cfCp49YDCPlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_out=bert(\n",
        "    input_ids=t[\"input_ids\"],\n",
        "    attention_mask=t[\"attention_mask\"],\n",
        "    token_type_ids=t[\"token_type_ids\"])\n",
        "#an easy way to say the above would be bert(**t)\n"
      ],
      "metadata": {
        "id": "KkunPLnt2QP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "that's it, this is how you call BERT, now let's see what it gave us (not hard to figure out it is really a dictionary)"
      ],
      "metadata": {
        "id": "w93TRT3rCloK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_out.keys()"
      ],
      "metadata": {
        "id": "CvGUp99n3e9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* last_hidden_state: the last layer of the encoder\n",
        "* pooler_output: the `tanh` layer on top of `[CLS]`"
      ],
      "metadata": {
        "id": "m6X-qckrC1VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before you run this, stop to think:\n",
        "# What will the shape be? How many dimensions? 1? 2? 3? more? And their approximate sizes?\n",
        "# make a guess, see if it matches\n",
        "bert_out.last_hidden_state.shape"
      ],
      "metadata": {
        "id": "vlaUdarn3Yj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And here? What will the shape be?\n",
        "bert_out.pooler_output.shape"
      ],
      "metadata": {
        "id": "qIyI5ZfG3sBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT: masked language modelling output\n",
        "\n",
        "* Not much we can do with the above\n",
        "* But BERT is trained to predict masked words, let's try!"
      ],
      "metadata": {
        "id": "AXhX93tbD2tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Have a look at HuggingFace automodels documentation to see what types of automodels there are\n",
        "# (Do you notice how the complaint of unused weights got a lot shorter? Why do you think that is?)\n",
        "bert=transformers.AutoModelForPreTraining.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "iQy5gL0Z4VFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tell the model it is not really being trained (disables dropout for example)\n",
        "# I do not think this is needed but am playing it safe, the docs say it is put to eval mode upon load: https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.from_pretrained.config\n",
        "bert=bert.eval()"
      ],
      "metadata": {
        "id": "RAQmsa5b41sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can again run the model, and we will see the output is quite different!"
      ],
      "metadata": {
        "id": "LuFpVyBpFXjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_out=bert(**t)\n",
        "bert_out.keys()"
      ],
      "metadata": {
        "id": "HzHG0jn44-PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What do you think these shapes will be?\n",
        "print(\"Logits\",bert_out[\"prediction_logits\"].shape)\n",
        "print(\"Seq relationship logits\",bert_out[\"seq_relationship_logits\"].shape)"
      ],
      "metadata": {
        "id": "IYEFMKuK5A8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cross-check\n",
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "8avjXIqT5L9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...now let's see how well this works for the masked word prediction...\n",
        "* we need to find the most likely predicted words\n",
        "* which can be achieved by arg-sorting the predictions and picking top N words\n",
        "* this is easy and we have done this kind of stuff before\n",
        "* now let's try straight in torch without a roundtrip to numpy"
      ],
      "metadata": {
        "id": "XtSGBan5QaXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top20=torch.argsort(predictions,dim=2,descending=True)[:,:,:20] #why dim=2? what does [:,:,:20] do?\n",
        "print(top20)"
      ],
      "metadata": {
        "id": "t8lxob7T8sIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[0])\n",
        "print(\"Guesses:\",tokenizer.decode(top20[0,4]))"
      ],
      "metadata": {
        "id": "aHsdKBuU9H-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ...in one block..."
      ],
      "metadata": {
        "id": "M0iadvxCSRNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts=[\"Dogs like to [MASK] cats. They are cute.\"]\n",
        "t=tokenizer(texts,padding=True, truncation=True, return_tensors=\"pt\")\n",
        "bert_out=bert(**t)\n",
        "top20=torch.argsort(bert_out[\"prediction_logits\"],dim=2,descending=True)[:,:,:20]\n",
        "print(\"Guesses:\",tokenizer.decode(top20[0,4]))"
      ],
      "metadata": {
        "id": "4XhmgLMgRubI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(t)\n",
        "print(tokenizer.mask_token_id)"
      ],
      "metadata": {
        "id": "Nmi_zDnsTKbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASKS\n",
        "\n",
        "As an exercise, you can try to solve any subset of the following:\n",
        "\n",
        "1. How good is BERT at the masked language modelling (MLM) task? Feed random texts e.g. from the IMDB dataset, mask a random token at a time, and check: did BERT predict it correctly?\n",
        "2. If you did (1), can you answer did BERT predict it correctly in top-5?\n",
        "3. Try can you do better. Make yourself a program which picks random texts from one of the datasets we used in this course and produces two files: one with segments of texts with one [MASK] and one with the correct answers. Then try to guess the words without looking at the latter file and then compare your answers with the correct ones. How well did you do?\n"
      ],
      "metadata": {
        "id": "jZ3CmFs0VYed"
      }
    }
  ]
}