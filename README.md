# Deep Learning in Human Language Technology course

Materials for the University of Turku course *TKO_8965 Deep Learning in Human Language Technology* (previously named *TKO_2101 Natural Language Processing*).

[UTU Moodle page](https://moodle.utu.fi/course/view.php?id=18315)

# How to attend the course

* We expect you to **take notes during the lecture**. The online materials support, but do not replace the lectures. It is highly unlikely you can pass the course by simply reading through the online materials.
* If you cannot make it to a lecture, find a friend who can share their notes with you.

# Computers and accounts

* The course takes place in the linux classroom. You will receive user accounts and passwords during the first demo session.
* See [running_code.ipynb](running_code.ipynb) for details on the various machines

# Feed-forward NNs and the BoW model

Bag of words text classification with neural networks. On the lectures, we work our way through basic neural network models, their training, and application to classification.

* [Bag-of-words text classification - notebook](bow_classifier.ipynb)
* [Classifier word vector analysis - notebook](bow_classifier_features.ipynb)
* [Word embeddings - notebook](word_embeddings.ipynb)
* [BoW classifier with pretrained word embeddings - notebook](bow_classifier_embeddings_simpler.ipynb)

# Convolutional Neural Networks

Convolutional neural networks and their use in natural language processing.

* [Sequence to Label with CNNs - notebook](seq2label_conv.ipynb)
* [CNN filter interpretation - notebook](cnn_filters.ipynb)
* [Pytorch CNN model - notebook](cnn_model_pytorch.ipynb)

# Recurrent Neural Networks

Introduction to recurrent neural networks and applications to various NLP tasks.

* [Recurrent neural networks - slides](slides/Recurrent_neural_networks.pdf)
* [Long short-term memory - slides](slides/Long_short-term_memory.pdf)
* [Text classification with recurrent neural networks - notebook](rnn_text_classification.ipynb)
* [Text generation with recurrent neural networks - notebook](rnn_text_generation.ipynb)
* [Named entity recognition with recurrent neural networks - notebook](rnn_named_entity_recognition.ipynb)

# Sequence to sequence and attention

Encoder-decorer and sequence-to-sequence architectures and introduction to neural attention.

* [Sequence to sequence and neural attention - slides](slides/Sequence_to_sequence_and_attention.pdf)
* [Sequence to sequence date normalization - notebook](seq2seq_dates.ipynb)
* [Sequence to sequence English to katakana translation - notebook](seq2seq_katakana.ipynb)
* [Neural machine translation with attention - TensorFlow tutorial](https://www.tensorflow.org/tutorials/text/nmt_with_attention)

# Transformer and transfer learning

* [Transformer and transfer learning - slides](slides/Transformer_and_transfer_learning.pdf)
* [Deep neural language models - slides](slides/Deep_neural_language_models.pdf)
* [Text classification with BERT - notebook](bert_text_classification.ipynb)
* [Sequence labeling with BERT - notebook](bert_sequence_labeling.ipynb)

# Applications

* [Sequence to sequence applications - notebook](seq2seq_applications.ipynb)
* [Crosslingual sentence representations - slides](slides/Crosslingual_sentence_representations.pdf)
* [Laser and BERT embeddings - notebook](laser.ipynb)
* [Inference as benchmark - slides](slides/Inference_as_benchmark.pdf)
* [Additional information on evaluation and paraphrase datasets](Paraphrase_corpora.pdf)
