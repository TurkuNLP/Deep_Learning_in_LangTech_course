{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDKeu9y2STbPTRhc6H9LPD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/dl_in_hlt_2025_exercise_2_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example solution to exercise task 2\n",
        "\n",
        "Task description:\n",
        "\n",
        "> ### Load a dataset, and tokenize and vectorize the data\n",
        ">\n",
        "> In this exercise, the task is to load a text classification dataset using the `datasets` Python library, and tokenize and vectorize the loaded data using the tokenizer created in the exercise task 1. This exercise builds towards a full model training notebook.\n",
        ">\n",
        "> 1) Load the `imdb` movie review dataset using the `datasets` Python library. Here is a [helper notebook](https://github.com/TurkuNLP/intro-to-nlp/blob/master/datasets_introduction.ipynb) from the Introduction to Language Technology course in case you are not familiar with `datasets` or need a reminder.\n",
        "\n",
        "We'll follow the initial steps in the referenced notebook to install and import the `datasets` library."
      ],
      "metadata": {
        "id": "883jnLeato5I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTywuo3wtmyq"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets"
      ],
      "metadata": {
        "id": "7RhXvyvsuKv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the notebook further, we load the named dataset with the `load_dataset` function ([documentation](https://huggingface.co/docs/datasets/package_reference/loading_methods#datasets.load_dataset))."
      ],
      "metadata": {
        "id": "nhHhU7CiuZro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = 'imdb'\n",
        "\n",
        "dataset = datasets.load_dataset(DATASET_NAME)"
      ],
      "metadata": {
        "id": "QgZJuIwpuX_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a quick look at the contents."
      ],
      "metadata": {
        "id": "djzI0kIvu2Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "qr3zKg5guwB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `dataset` object we loaded is an instance of the `DatasetDict` class, which is a dictionary-like class that maps strings such as `train` to `Dataset` objects. So, we can reference one of the `Dataset` objects in the `DatasetDict` like this:"
      ],
      "metadata": {
        "id": "n95wOHbsu6VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']"
      ],
      "metadata": {
        "id": "qeoabIeIOdGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each item in the dataset contains a `text` and a `label`, as we would expect for a dataset for text classification. We can look at an individual example from the dataset by indexing it like a list:"
      ],
      "metadata": {
        "id": "ToNradTLQva4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "wYK8DBOlQ8u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that for the `imdb` dataset we have the conventional `train` and `test` parts and an `unsupervised` part that's somewhat specific to this dataset.  If you're interested in the intended role of this data, you can read about it in the [original paper introducing the IMDB dataset](https://aclanthology.org/P11-1015.pdf)."
      ],
      "metadata": {
        "id": "4Gno2GM0OdfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task description:\n",
        "\n",
        "> 2) Tokenize and vectorize the dataset using a tokenizer created in the exercise task 1. The tokenizer can be either monolingual English or multilingual (e.g. `bert-base-cased` or `bert-base-multilingual-cased`). The outcome of a tokenized and vectorized example should look something like this (some tokenizers do not produce token_type_ids):\n",
        ">\n",
        ">```\n",
        ">{'attention_mask': [1, 1, 1, ... , 1],\n",
        "> 'input_ids': [101, 146, 12765, ... , 102],\n",
        "> 'token_type_ids': [0, 0, 0, ..., 0]}\n",
        ">```\n",
        ">\n",
        "> Hint: To tokenize and vectorize the whole dataset, write a function which receives one example, and returns it's tokenized+vectorized version. Apply this function to each example in the dataset using `dataset.map()`."
      ],
      "metadata": {
        "id": "4oB13jIFvjmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll first grab the a tokenizer as in exercise task 1:"
      ],
      "metadata": {
        "id": "7nYCYhK8v36x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet transformers"
      ],
      "metadata": {
        "id": "wQBVfGGzwI5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "TOKENIZER_NAME = 'bert-base-cased'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)"
      ],
      "metadata": {
        "id": "OUXyU1W9v_55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have already seen on the lecture, we can get tokenized and vectorized output of the type that this task requires simply by invoking the tokenizer directly:"
      ],
      "metadata": {
        "id": "GPDl8-1qwYDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer('Hello world!')"
      ],
      "metadata": {
        "id": "7uxiE0E9u4gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The task asks us to process the whole dataset using the [`Dataset.map()`](https://huggingface.co/docs/datasets/en/process#map) function. We could do this for each of the `Dataset` object contained in the top-level `DatasetDict` object separately (e.g. in a loop), but for convenience `DatasetDict` also implements functions such as `map()` ([documentation](https://huggingface.co/docs/datasets/en/process#process-multiple-splits)) in just this way, calling them for all of its contained `Dataset` objects. We can thus ignore the difference between `Dataset` and `DatasetDict` here and call `map()` directly with the object we have loaded."
      ],
      "metadata": {
        "id": "cyFvralpO6Zv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As suggested by the hint, let's write a function that invokes the tokenizer. We'll first try to make this work with just text strings."
      ],
      "metadata": {
        "id": "Xlg7ZAlOwn1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_string(text):\n",
        "  return tokenizer(text)"
      ],
      "metadata": {
        "id": "Lg7YPOY5wmM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This works as we would expect for strings"
      ],
      "metadata": {
        "id": "BlsGGDEZP35r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_string('Hello world!')"
      ],
      "metadata": {
        "id": "N84hk0EGP6bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's try to use the `map` function ([documentation](https://huggingface.co/docs/datasets/en/process#map)) referenced in the hint to apply that to the dataset."
      ],
      "metadata": {
        "id": "dGOjYVIow5X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.map(tokenize_string)"
      ],
      "metadata": {
        "id": "rTJQCZTjw3aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That didn't work because the tokenizer expected a string (or lists or lists of lists of strings). What do we have instead? (The `sys.exit` below is just to avoid the callback.)"
      ],
      "metadata": {
        "id": "LhrgNBYCxY7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "def debug_tokenize(example):\n",
        "  print(type(example), example)\n",
        "  sys.exit()\n",
        "\n",
        "dataset.map(debug_tokenize)"
      ],
      "metadata": {
        "id": "XroxgHRAxIhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like we have a dictionary with `text` and `label`, which makes sense -- this is what each example in the dataset contains. It's the text we want to tokenize, so let's try that."
      ],
      "metadata": {
        "id": "Rr2oyO_BzaXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "  return tokenizer(example['text'])\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize)\n",
        "\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "0hZJg6tsxz5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That appears to have worked as intended, with `DatasetDict.map()` applying our tokenization function to all of the included `Dataset` objects. Note and that the new `DatasetDict` returned by the function contains not only the fields returned by the tokenizer (`input_its`, `token_type_ids`, and `attention_mask`) but also the original ones (`text` and `label`).\n",
        "\n",
        "Note that the `map` function did not modify the original `DatasetDict` or the `Dataset` objects that it contains, so these are different:"
      ],
      "metadata": {
        "id": "QvKZmo790tBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Original:', dataset)\n",
        "print('Tokenized:', tokenized_dataset)"
      ],
      "metadata": {
        "id": "K0NKbOWhR0Kj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}