{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': 'pos', 'text': 'In the early 1970s, many of us who had embraced hippy/acid/alternative culture wholeheartedly, had realised that mainstream society was not going to change in the way we thought it would. For me this film defined the tension of \\\\now what?\\\\\" that many of the people I knew felt at that time. Do we head for the commune and create our own vision of utopia or do we radicalize and push for change?  From the futile gestures to police, the radical rhetoric, the music of Pink Floyd, to the love-making and the explosive images in the desert - so much is in there. It captures well a significant moment in time.\"'}\n",
      "['In the early 1970s, many of us who had embraced hippy/acid/alternative culture wholeheartedly, had realised that mainstream society was not going to change in the way we thought it would. For me this film defined the tension of \\\\now what?\\\\\" that many of the people I knew felt at that time. Do we head for the commune and create our own vision of utopia or do we radicalize and push for change?  From the futile gestures to police, the radical rhetoric, the music of Pink Floyd, to the love-making and the explosive images in the desert - so much is in there. It captures well a significant moment in time.\"', \"...the first film I had to walk out on. And it was the cast and crew pre-screening (Not that I was involved, I hasten to add). I made it through the first hour, so I reckon I'm just qualified to comment, but that was my limit.  Like other comments here, how did this get through any kind of QA. An accumulation of the very worst in dialogue, the epitome of wooden acting, awful casting, all wrapped together without a plot.  Tara Fitzgerald's casting was bizarre, almost comic. She possesses the worst Russian accent in movie history.  As I left the screening, the director and producers were drinking in a bar outside the cinema. They obviously couldn't sit through it again either.  \"]\n",
      "['pos', 'neg']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "with open(\"data/imdb_train.json\") as f:\n",
    "    data=json.load(f)\n",
    "random.shuffle(data) \n",
    "print(data[0])\n",
    "\n",
    "# We need to gather the texts, into a list\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"class\"] for one_example in data]\n",
    "print(texts[:2])\n",
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'early', '1970s', 'many', 'of', 'us', 'who', 'had', 'embraced', 'hippy', 'acid', 'alternative', 'culture', 'wholeheartedly', 'had', 'realised', 'that', 'mainstream', 'society', 'was', 'not', 'going', 'to', 'change', 'in', 'the', 'way', 'we', 'thought', 'it', 'would', 'For', 'me', 'this', 'film', 'defined', 'the', 'tension', 'of', 'now', 'what', 'that', 'many', 'of', 'the', 'people', 'knew', 'felt', 'at', 'that', 'time', 'Do', 'we', 'head', 'for', 'the', 'commune', 'and', 'create', 'our', 'own', 'vision', 'of', 'utopia', 'or', 'do', 'we', 'radicalize', 'and', 'push', 'for', 'change', 'From', 'the', 'futile', 'gestures', 'to', 'police', 'the', 'radical', 'rhetoric', 'the', 'music', 'of', 'Pink', 'Floyd', 'to', 'the', 'love', 'making', 'and', 'the', 'explosive', 'images', 'in', 'the', 'desert', 'so', 'much', 'is', 'in', 'there', 'It', 'captures', 'well', 'significant', 'moment', 'in', 'time']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy\n",
    "analyzer=CountVectorizer(lowercase=False).build_analyzer() # includes tokenizer and preprocessing\n",
    "print(analyzer(texts[0]))\n",
    "\n",
    "\n",
    "#vectorizer=vectorizer.fit(texts) # learns a vocabulary dictionary\n",
    "#print(\"Vocabulary size:\",len(vectorizer.vocabulary_))\n",
    "#print(\"First 5 items in the vocabulary:\",list(vectorizer.vocabulary_.keys())[:5])\n",
    "#print(\"How many words are recognized from the data:\",numpy.count_nonzero(vectorizer.transform([\"i went today to new_york\"]).todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words from embedding model: 50000\n",
      "First 50 words: ['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said', 'was', 'the', 'at', 'not', 'as', 'it', 'be', 'from', 'by', 'are', 'I', 'have', 'he', 'will', 'has', '####', 'his', 'an', 'this', 'or', 'their', 'who', 'they', 'but', '$', 'had', 'year', 'were', 'we', 'more', '###', 'up', 'been', 'you', 'its', 'one', 'about', 'would', 'which', 'out']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "vector_model=KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin\", binary=True, limit=50000)\n",
    "\n",
    "# sort based on the index to make sure they are in the correct order\n",
    "# lowercase everything because that is the dafeault setting in CountVectorizer\n",
    "words=[k for k,v in sorted(vector_model.vocab.items(), key=lambda x:x[1].index)]\n",
    "print(\"Words from embedding model:\",len(words))\n",
    "print(\"First 50 words:\",words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words from embedding model: 49412\n"
     ]
    }
   ],
   "source": [
    "# init the vectorizer vocabulary using words from the embedding model\n",
    "\n",
    "def init_vocabulary(vocab, text, text_analyzer):\n",
    "    for word in analyzer(text):\n",
    "        vocab.setdefault(word, len(vocab))\n",
    "    return vocab\n",
    "\n",
    "words_txt=\" \".join(words)\n",
    "vocabulary={\"<SPECIAL>\": 0} # zero has a special meaning, prevent using it for a normal word\n",
    "vocabulary=init_vocabulary(vocabulary,words_txt,analyzer)\n",
    "print(\"Words from embedding model:\",len(vocabulary))\n",
    "#for word in words:\n",
    "#    if word not in vocab:\n",
    "#        print(word)\n",
    "#    vocab.setdefault(word, len(vectorizer.vocabulary_)) # setdefault adds the word if it does not already exist\n",
    "#vectorizer.vocabulary_=vocab\n",
    "#print(\"Vocabulary size:\",len(vectorizer.vocabulary_))\n",
    "#print(\"First 5 items in the vocabulary:\",list(vectorizer.vocabulary_.keys())[:5])\n",
    "#print(\"How many words are recognized from the data:\",numpy.count_nonzero(vectorizer.transform([\"i went today to new_york\"]).todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in vocabulary: 111151\n",
      "Vectorized data shape: (25000,)\n",
      "Firs example vectorized: [65, 10, 262, 49412, 122, 49413, 152, 28, 31, 8427, 49414, 14932, 2116, 1983, 31886, 31, 49415, 3, 5220, 1928, 9, 12, 116, 49416, 399, 1, 10, 123, 34, 542, 14, 42, 178, 157, 25, 659, 3634, 10, 6551, 49413, 90, 76, 3, 122, 49413, 10, 63, 1001, 1002, 11, 3, 54, 813, 34, 349, 2, 10, 46989, 49417, 778, 66, 231, 2602, 49413, 49418, 26, 53, 34, 49419, 49417, 1634, 2, 399, 1202, 10, 20306, 14409, 49416, 174, 10, 5975, 8014, 10, 603, 49413, 15059, 8931, 49416, 10, 713, 297, 49417, 10, 6733, 2589, 1, 10, 6667, 78, 139, 4, 1, 67, 46, 12679, 104, 789, 1282, 1, 54]\n"
     ]
    }
   ],
   "source": [
    "def vectorizer(vocab, texts):\n",
    "    vectorized_data=[] # turn text into numbers based on our vocabulary mapping\n",
    "    for one_example in texts:\n",
    "        vectorized_example=[]\n",
    "        for word in analyzer(one_example):\n",
    "            vocab.setdefault(word, len(vocab)) # add word to out vocabulary if it does not exist\n",
    "            vectorized_example.append(vocab[word])\n",
    "        vectorized_data.append(vectorized_example)\n",
    "    \n",
    "    vectorized_data=numpy.array(vectorized_data) # turn python list into numpy matrix\n",
    "    return vectorized_data\n",
    "\n",
    "vectorized_data=vectorizer(vocabulary, texts)\n",
    "\n",
    "# now vectorized data is in the same as feature_matrix, but in slightly different format\n",
    "print(\"Words in vocabulary:\",len(vocabulary))\n",
    "print(\"Vectorized data shape:\",vectorized_data.shape)\n",
    "print(\"Firs example vectorized:\",vectorized_data[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_numbers shape= (25000,)\n",
      "class_numbers [1 0 1 ... 1 1 1]\n",
      "class labels ['neg' 'pos']\n",
      "classes_1hot [[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#feature_matrix=vectorizer.transform(texts)\n",
    "#print(feature_matrix.shape)\n",
    "\n",
    "# labels\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "label_encoder=LabelEncoder() #Turns class labels into integers\n",
    "one_hot_encoder=OneHotEncoder(sparse=False) #Turns class integers into one-hot encoding\n",
    "class_numbers=label_encoder.fit_transform(labels)\n",
    "print(\"class_numbers shape=\",class_numbers.shape)\n",
    "print(\"class_numbers\",class_numbers)\n",
    "print(\"class labels\",label_encoder.classes_)\n",
    "#And now yet the one-hot encoding\n",
    "classes_1hot=one_hot_encoder.fit_transform(class_numbers.reshape(-1,1))\n",
    "print(\"classes_1hot\",classes_1hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained vectors for 48923 words.\n",
      "Shape of pretrained embeddings: (111151, 300)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_pretrained_embeddings(vocab, embedding_model):\n",
    "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
    "    import string\n",
    "    pretrained_embeddings=numpy.zeros((len(vocab),embedding_model.vectors.shape[1])) # initialize new matrix (words x embedding dim)\n",
    "    found=0\n",
    "    for word,idx in vocab.items():\n",
    "        if word in embedding_model.vocab:\n",
    "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
    "            found+=1\n",
    "            \n",
    "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
    "    return pretrained_embeddings\n",
    "\n",
    "pretrained=load_pretrained_embeddings(vocabulary, vector_model)\n",
    "print(\"Shape of pretrained embeddings:\",pretrained.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape: (25000, 2366)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 2366)              0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 2366, 200)         22230200  \n",
      "_________________________________________________________________\n",
      "lambda_5 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 22,230,602\n",
      "Trainable params: 22,230,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "22500/22500 [==============================] - 115s 5ms/step - loss: 0.6456 - acc: 0.6564 - val_loss: 0.5355 - val_acc: 0.8424\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Activation\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "vectorized_data_padded=pad_sequences(vectorized_data)#, maxlen=100, padding='post', truncating='post')\n",
    "print(\"New shape:\", vectorized_data_padded.shape)\n",
    "example_count,sequence_len=vectorized_data_padded.shape\n",
    "example_count,class_count=classes_1hot.shape\n",
    "\n",
    "#example_count,feature_count=feature_matrix.shape\n",
    "\n",
    "#example_count,class_count=classes_1hot.shape\n",
    "\n",
    "\n",
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary),200,mask_zero=True)(inp)\n",
    "sums=Lambda(lambda s: K.sum(s, axis=1), output_shape=lambda s: (s[0],s[2]))(embeddings) # custom layer to sum all embeddings\n",
    "tanh=Activation(\"tanh\")(sums)\n",
    "\n",
    "outp=Dense(class_count,activation=\"softmax\")(tanh)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "hist=model.fit(vectorized_data_padded,classes_1hot,batch_size=100,verbose=1,epochs=1,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
