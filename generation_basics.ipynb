{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQoK0iVXvQWmijmbBROaqG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/generation_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation basics\n",
        "\n",
        "This notebook illustrates some parts that go into text generation using the `pipeline` class from the `transformers` library."
      ],
      "metadata": {
        "id": "upiVhm3M7i7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First install the [transformers](https://huggingface.co/docs/transformers/index) package."
      ],
      "metadata": {
        "id": "-ImKmG7j70Z1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jS-wINx56E5"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, load a `pipeline` for text generation with a small model."
      ],
      "metadata": {
        "id": "jIPvUiXkoK3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "MODEL_NAME = 'HuggingFaceTB/SmolLM-135M'\n",
        "\n",
        "pipe = pipeline('text-generation', model=MODEL_NAME)"
      ],
      "metadata": {
        "id": "8Haw6sWQo9s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can conveniently generate text using the high-level abstraction that `pipeline` provides:"
      ],
      "metadata": {
        "id": "fuRhqTzLuTMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'The capital of Finland is'\n",
        "\n",
        "print(pipe(prompt)[0]['generated_text'])"
      ],
      "metadata": {
        "id": "GyvybclMufGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For simplicity, let's look at generating one word using greedy decoding, i.e. simply selecting the word that's most likely according to the model."
      ],
      "metadata": {
        "id": "sC7CXiDFvlpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'do_sample': False,\n",
        "    'max_new_tokens': 1,\n",
        "}\n",
        "\n",
        "print(pipe(prompt, **params)[0]['generated_text'])"
      ],
      "metadata": {
        "id": "pLCvXb0Vv1GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's look at what's going on behind the `pipeline` abstraction. First, here's the model"
      ],
      "metadata": {
        "id": "795RV2T0wLnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = pipe.model\n",
        "print(model)"
      ],
      "metadata": {
        "id": "d41xTKy7wa-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model doesn't actually deal with text directly, but rather with token indices. The mapping between running text and token indices is implemented by a tokenizer."
      ],
      "metadata": {
        "id": "BliQAAUiwrhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = pipe.tokenizer"
      ],
      "metadata": {
        "id": "4F6Ss_LUwclI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at the mapping between our prompt and the token indices."
      ],
      "metadata": {
        "id": "OUgQ0FZZxBKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ = tokenizer(prompt)\n",
        "\n",
        "print(input_)"
      ],
      "metadata": {
        "id": "ajSrSCJRw5xK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's the actual input to the model, and `input_ids` are the token indices. (We can ignore the `attention_mask` here.)\n",
        "\n",
        "The tokenizer can map these back to text"
      ],
      "metadata": {
        "id": "pcLaMwi_xN9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_ids_to_tokens(input_.input_ids))"
      ],
      "metadata": {
        "id": "6W8aczlQxqqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(The `Ä ` there encodes space; this representation is a minor quirk of the GPT tokenizer.)\n",
        "\n",
        "Because the token ids represent both visible characters and space, the full input string can be reconstructed accurately:"
      ],
      "metadata": {
        "id": "MWu8qcduyEIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(input_.input_ids))"
      ],
      "metadata": {
        "id": "J47fG7k6yA9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can invoke the model directly with the encoded input. Here we need to ask the tokenizer to generate pytorch tensors due to some implementation details, but the information content is the same."
      ],
      "metadata": {
        "id": "IDM-b-jwyraj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "print(input_)"
      ],
      "metadata": {
        "id": "fUFYYdcYzrYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**input_)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "sXRPVpQWyngG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary output of the model are the logits, which correspond to unnormalized scores for each token. We're interested in the scores for the last token, which are used to predict the next one. (The first dimension here is for the batch, and we have a batch of one.)"
      ],
      "metadata": {
        "id": "9qRlDUDY0DOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = output.logits[0][-1]\n",
        "print(logits.shape)\n",
        "print(logits)"
      ],
      "metadata": {
        "id": "D92PA1Q7y50u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For greedy decoding, we can just take the argmax, which gives us the index of the most likely next word."
      ],
      "metadata": {
        "id": "0Y0IJMkO1pPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits.argmax()"
      ],
      "metadata": {
        "id": "PHHoDolY0Szv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens([logits.argmax()])"
      ],
      "metadata": {
        "id": "z8THCuvx0Xnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we wanted to continue generating more than one word, we would simply append this index to `input_ids` and invoke the model again."
      ],
      "metadata": {
        "id": "70_YUeW615VU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "bTLYcNRdGfbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings\n",
        "\n",
        "The model operates with continuous representations rather than discrete word identifiers. To map the `input_ids` to a continuous representation, the first step in the model application is to look up a learned (context-independent) embedding. These are similar to the embeddings generated by methods such as `word2vec`.\n",
        "\n",
        "(We already saw an instance of mapping from the continuous representations to discrete IDs on the output through `argmax`)\n",
        "\n",
        "Let's have a quick look at the embeddings for this model."
      ],
      "metadata": {
        "id": "buiuBmy8Gws8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = model.get_input_embeddings()\n",
        "print(embedding)"
      ],
      "metadata": {
        "id": "R_0PzoN4JfHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the first value is the number of tokens in the model vocabulary and the second the hidden dimension of the model."
      ],
      "metadata": {
        "id": "Ka6n23LSKBza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write a few functions for getting IDs and embeddings for individual words and look at the IDs and embeddings for the words \"dog\", \"cat\", and \"hat\"."
      ],
      "metadata": {
        "id": "VZNYJlVwLg7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_id(word):\n",
        "  ids = tokenizer(' ' + word).input_ids    # add initial space\n",
        "  assert len(ids) == 1, f'multiple tokens for {word}'\n",
        "  return ids[0]\n",
        "\n",
        "dog_id = get_id('dog')\n",
        "cat_id = get_id('cat')\n",
        "hat_id = get_id('hat')"
      ],
      "metadata": {
        "id": "vV462I1dGw-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The IDs themselves are arbitrary and meaningless outside of their reference to the embedding matrix:"
      ],
      "metadata": {
        "id": "2WTuOCo2MySv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dog_id, cat_id, hat_id)"
      ],
      "metadata": {
        "id": "pRRRNQGnNa32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's grab the corresponding embeddings"
      ],
      "metadata": {
        "id": "__HZxPksNh4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def get_embedding(id_):\n",
        "  return embedding(torch.tensor(id_))\n",
        "\n",
        "dog_emb = get_embedding(dog_id)\n",
        "cat_emb = get_embedding(cat_id)\n",
        "hat_emb = get_embedding(hat_id)"
      ],
      "metadata": {
        "id": "Cu5KtNMtIfjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are vectors of the hidden dimensionality of the model"
      ],
      "metadata": {
        "id": "uo1ls3g6NnLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dog_emb.shape)"
      ],
      "metadata": {
        "id": "cb74wpqBNsOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embeddings cannot be interpreted in isolation, but are \"understood\" by the model."
      ],
      "metadata": {
        "id": "ate3zcjsNuuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dog_emb[:100])"
      ],
      "metadata": {
        "id": "8r7pvecsOIBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embeddings can also be used e.g. to compare the similarity of (context-free) word representations:"
      ],
      "metadata": {
        "id": "gR5pmOScOLI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import cosine_similarity\n",
        "\n",
        "def compare(emb1, emb2):\n",
        "  return cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n",
        "\n",
        "print(compare(dog_emb, cat_emb))\n",
        "print(compare(cat_emb, hat_emb))"
      ],
      "metadata": {
        "id": "MG1vMXRwLN8q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}