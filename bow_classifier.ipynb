{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words document classification\n",
    "\n",
    "* BoW is the simplest way to do classification: Feature vector goes in, decision falls out.\n",
    "\n",
    "* Feature vector: a vector with as many dimensions as we have unique features, and a non-zero value set for every feature present in our example\n",
    "* Binary features: 1/0\n",
    "\n",
    "In the following we work with the IMDB data, have a look on [how to read it in](read_imdb.ipynb). Here we just read the ready data in.\n",
    "\n",
    "# IMDB data\n",
    "\n",
    "* Movie review sentiment positive/negative\n",
    "* Some 25,000 examples, 50:50 split\n",
    "* Current state-of-the-art is about 95% accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"I'm not a Steve Carell fan however I like this movie about Dan, an advice columnist, who goes to his parents house for a stay with his kids and ends up falling in love with his brother's girlfriend. Its a story thats been told before, but not like this. There are simply too many little bits that make the film better than it should be. The cast is wonderful, and even if Carell is not my cup of tea, he is quite good as the widower who's suppose to know everything but finds that knowing is different than feeling and that sometimes life surprises you. At times witty and wise in the way that an annoying Hallmark card can be, the film still some how manages to grow on you and be something more than a run of the mill film. Worth a look see\", 'class': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "with open(\"data/imdb_train.json\") as f:\n",
    "    data=json.load(f)\n",
    "random.shuffle(data) #play it safe!\n",
    "print(data[0]) #Every item is a dictionary with `text` and `class` keys, here's the first one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn on this data, we will need a few steps:\n",
    "\n",
    "* Build a data matrix with dimensionality (number of examples, number of possible features), and a value for each feature, 0/1 for binary features\n",
    "* Build a class label matrix (number of examples, number of classes) with the correct labels for the examples, setting 1 for the correct class, and 0 for others\n",
    "\n",
    "It is quite useless to do all this ourselves, so we will use ready-made classes and functions mostly from scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm not a Steve Carell fan however I like this movie about Dan, an advice columnist, who goes to his parents house for a stay with his kids and ends up falling in love with his brother's girlfriend. Its a story thats been told before, but not like this. There are simply too many little bits that make the film better than it should be. The cast is wonderful, and even if Carell is not my cup of tea, he is quite good as the widower who's suppose to know everything but finds that knowing is different than feeling and that sometimes life surprises you. At times witty and wise in the way that an annoying Hallmark card can be, the film still some how manages to grow on you and be something more than a run of the mill film. Worth a look see\", 'Once in a while, a film comes along that raises the bar for every other film in its genre. A film of this caliber will influence many films following its release for years to come. `A Chinese Ghost Story\\' falls in this category. It is arguably one of the best horror films made during the 1980\\'s; possibly one of the best ever made.  The filmmakers have crafted a movie that appeals to every horror fan. The story is engrossing and original. The villains are appropriately menacing and frightening. The sets are creepy and atmospheric. There is even a little blood and gore to satisfy the splatter fan of the house. But don\\'t let the `horror\\' label scare you off, if you\\'re not a fan of the genre. This film easily fits into many different categories.  The screenwriter has deftly blended the drama, comedy, horror, kung fu, and romance genres into a delicious deluxe cinematic pizza. `A Chinese Ghost Story\\' is a beautiful epic love story told, thankfully, without the gratuitous nudity and/or explicit sex scenes that have ruined many Hollywood `love stories\\'. Those put off by the romantic elements of the story can sit back and revel in the fast-paced swordplay and `wire-fu\\'. If that\\'s not enough, actors Leslie Cheung and Wu Ma provide enough humorous situations to satiate your appetite for comedy. This film offers something for every film fan.  Director Siu-Tung Ching and Producer Tsui Hark assembled a truly amazing cast for this film. Leslie Cheung proves that he is not only a gifted actor, but also a talented singer and a charming physical comedian. I cannot possibly think of a performer other than Cheung who could have portrayed Ling Choi Sin better (except maybe Chow Yun Fat). Joey Wang is enchanting as Lit Su Seen, the enslaved spirit who steals the heart of Cheung\\'s character. Her portrayal of the title character is truly haunting and memorable. Wu Ma is hilarious as the cantankerous Taoist who aids the young lovers.  On technical level, this film is very impressive, even by today\\'s standards. The direction is superb. I wish that today\\'s Hollywood executives would seek out talented artists like Siu-Tung Ching rather falling back on the usual MTV video or Pepsi commercial `directors\\'. The cinematography is gorgeous. You have to commend any cinematographer who can make a film look good when most of its pivotal scenes take place in the dead of night. The special effects make-up is top-notch. In fact, most of the creature effects in this film blow away the shoddy CGI ghouls and goblins that have become commonplace in modern horror films.  Since its release, \\\\A Chinese Ghost Story\\\\\" has spawned two worthy sequels, a full-length animated movie, and countless imitations. None of the films that followed it or copied it were able to capture the magic of this classic, however. This film is required viewing for any horror fan or just anyone looking for great way to spend 95 minutes of your time. 10 out 10.  \"']\n",
      "['pos', 'pos']\n"
     ]
    }
   ],
   "source": [
    "# We need to gather the texts, into a list\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"class\"] for one_example in data]\n",
    "print(texts[:2])\n",
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape= (25000, 74849)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,1))\n",
    "feature_matrix=vectorizer.fit_transform(texts)\n",
    "print(\"shape=\",feature_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the feature matrix done! Next thing we need is the class labels to be predicted in one-hot encoding. This means:\n",
    "\n",
    "* one row for every example\n",
    "* one column for every possible class label\n",
    "* exactly one column has 1 for every example, corresponding to the desired class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_numbers shape= (25000,)\n",
      "class labels ['neg' 'pos']\n",
      "classes_1hot [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "label_encoder=LabelEncoder() #Turns class labels into integers\n",
    "one_hot_encoder=OneHotEncoder(sparse=False) #Turns class integers into one-hot encoding\n",
    "class_numbers=label_encoder.fit_transform(labels)\n",
    "print(\"class_numbers shape=\",class_numbers.shape)\n",
    "print(\"class labels\",label_encoder.classes_) #this will let us translate back from indices to labels\n",
    "#And now yet the one-hot encoding\n",
    "classes_1hot=one_hot_encoder.fit_transform(class_numbers.reshape(-1,1)) #running without reshape tells you to reshape\n",
    "print(\"classes_1hot\",classes_1hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data is ready, we need to build the network now\n",
    "* Input\n",
    "* Hidden Dense layer with some kind of non-linearity, and a suitable number of nodes\n",
    "* Output Dense layer with the softmax activation (normalizes output to distribution) and as many nodes as there are classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ginter/venv-jupyter/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "example_count,feature_count=feature_matrix.shape\n",
    "example_count2,class_count=classes_1hot.shape\n",
    "assert example_count==example_count2 #sanity check\n",
    "\n",
    "inp=Input(shape=(feature_count,))\n",
    "hidden=Dense(200,activation=\"tanh\")(inp)\n",
    "outp=Dense(class_count,activation=\"softmax\")(hidden)\n",
    "model=Model(inputs=[inp], outputs=[outp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...it's **this** simple...!\n",
    "\n",
    "Once the model is constructed it needs to be compiled, for that we need to know:\n",
    "* which optimizer we want to use (sgd is fine to begin with)\n",
    "* what is the loss (categorial_crossentropy for multiclass of the kind we have is the right choice)\n",
    "* which metrics to measure, accuracy is an okay choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"sgd\",loss=\"categorical_crossentropy\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A compiled model can be fitted on data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/10\n",
      "22500/22500 [==============================] - 13s 578us/step - loss: 0.5685 - acc: 0.7732 - val_loss: 0.4884 - val_acc: 0.8256\n",
      "Epoch 2/10\n",
      "22500/22500 [==============================] - 12s 533us/step - loss: 0.4382 - acc: 0.8363 - val_loss: 0.4158 - val_acc: 0.8488\n",
      "Epoch 3/10\n",
      "22500/22500 [==============================] - 13s 574us/step - loss: 0.3790 - acc: 0.8556 - val_loss: 0.3793 - val_acc: 0.8572\n",
      "Epoch 4/10\n",
      "22500/22500 [==============================] - 11s 495us/step - loss: 0.3440 - acc: 0.8658 - val_loss: 0.3572 - val_acc: 0.8604\n",
      "Epoch 5/10\n",
      "22500/22500 [==============================] - 12s 547us/step - loss: 0.3200 - acc: 0.8745 - val_loss: 0.3424 - val_acc: 0.8648\n",
      "Epoch 6/10\n",
      "22500/22500 [==============================] - 14s 613us/step - loss: 0.3013 - acc: 0.8815 - val_loss: 0.3323 - val_acc: 0.8652\n",
      "Epoch 7/10\n",
      "22500/22500 [==============================] - 12s 525us/step - loss: 0.2859 - acc: 0.8892 - val_loss: 0.3215 - val_acc: 0.8700\n",
      "Epoch 8/10\n",
      "22500/22500 [==============================] - 15s 660us/step - loss: 0.2732 - acc: 0.8948 - val_loss: 0.3156 - val_acc: 0.8720\n",
      "Epoch 9/10\n",
      "22500/22500 [==============================] - 13s 568us/step - loss: 0.2616 - acc: 0.8997 - val_loss: 0.3095 - val_acc: 0.8736\n",
      "Epoch 10/10\n",
      "22500/22500 [==============================] - 13s 558us/step - loss: 0.2515 - acc: 0.9041 - val_loss: 0.3061 - val_acc: 0.8740\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(feature_matrix,classes_1hot,batch_size=100,verbose=1,epochs=10,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8255999970436096, 0.8488000011444092, 0.8572000002861023, 0.8603999996185303, 0.8647999954223633, 0.8651999974250794, 0.8700000023841858, 0.8720000004768371, 0.8735999941825867, 0.8739999938011169]\n"
     ]
    }
   ],
   "source": [
    "print(hist.history[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We ran for 10 epochs of training\n",
    "* Made it to 88.7% accuracy on the validation and 94.9% accuracy on the training data\n",
    "\n",
    "* But we do not have the model saved, so let's fix that and get the whole thing done\n",
    "* What constitutes a model (ie what we need to run the model on new data)\n",
    "  - The feature dictionary in the vectorizer\n",
    "  - The list of classes in their correct order\n",
    "  - The structure of the network\n",
    "  - The weights the network learned\n",
    "\n",
    "* Do all these things, and run again. This time we also increase the number of epochs to 100, see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/100\n",
      "22500/22500 [==============================] - 19s 827us/step - loss: 0.5680 - acc: 0.7744 - val_loss: 0.4902 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49017, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 2/100\n",
      "22500/22500 [==============================] - 25s 1ms/step - loss: 0.4374 - acc: 0.8370 - val_loss: 0.4154 - val_acc: 0.8476\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49017 to 0.41538, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 3/100\n",
      "22500/22500 [==============================] - 33s 1ms/step - loss: 0.3786 - acc: 0.8541 - val_loss: 0.3788 - val_acc: 0.8560\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.41538 to 0.37884, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 4/100\n",
      "22500/22500 [==============================] - 23s 1ms/step - loss: 0.3436 - acc: 0.8667 - val_loss: 0.3569 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.37884 to 0.35686, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 5/100\n",
      "22500/22500 [==============================] - 25s 1ms/step - loss: 0.3196 - acc: 0.8739 - val_loss: 0.3415 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35686 to 0.34151, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 6/100\n",
      "22500/22500 [==============================] - 16s 700us/step - loss: 0.3008 - acc: 0.8815 - val_loss: 0.3308 - val_acc: 0.8700\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.34151 to 0.33081, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 7/100\n",
      "22500/22500 [==============================] - 18s 799us/step - loss: 0.2859 - acc: 0.8880 - val_loss: 0.3215 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.33081 to 0.32154, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 8/100\n",
      "22500/22500 [==============================] - 13s 593us/step - loss: 0.2727 - acc: 0.8938 - val_loss: 0.3157 - val_acc: 0.8752\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.32154 to 0.31567, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 9/100\n",
      "22500/22500 [==============================] - 12s 525us/step - loss: 0.2615 - acc: 0.8996 - val_loss: 0.3096 - val_acc: 0.8776\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.31567 to 0.30955, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 10/100\n",
      "22500/22500 [==============================] - 13s 558us/step - loss: 0.2513 - acc: 0.9035 - val_loss: 0.3052 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.30955 to 0.30522, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 11/100\n",
      "22500/22500 [==============================] - 12s 526us/step - loss: 0.2423 - acc: 0.9076 - val_loss: 0.3032 - val_acc: 0.8796\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.30522 to 0.30319, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 12/100\n",
      "22500/22500 [==============================] - 12s 550us/step - loss: 0.2345 - acc: 0.9119 - val_loss: 0.2991 - val_acc: 0.8832\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.30319 to 0.29911, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 13/100\n",
      "22500/22500 [==============================] - 12s 534us/step - loss: 0.2269 - acc: 0.9152 - val_loss: 0.2975 - val_acc: 0.8828\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.29911 to 0.29745, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 14/100\n",
      "22500/22500 [==============================] - 12s 550us/step - loss: 0.2194 - acc: 0.9183 - val_loss: 0.2960 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.29745 to 0.29598, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 15/100\n",
      "22500/22500 [==============================] - 12s 545us/step - loss: 0.2132 - acc: 0.9209 - val_loss: 0.2947 - val_acc: 0.8808\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.29598 to 0.29473, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 16/100\n",
      "22500/22500 [==============================] - 12s 543us/step - loss: 0.2070 - acc: 0.9248 - val_loss: 0.2936 - val_acc: 0.8848\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.29473 to 0.29359, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 17/100\n",
      "22500/22500 [==============================] - 12s 552us/step - loss: 0.2013 - acc: 0.9266 - val_loss: 0.2939 - val_acc: 0.8804\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/100\n",
      "22500/22500 [==============================] - 12s 538us/step - loss: 0.1958 - acc: 0.9293 - val_loss: 0.2938 - val_acc: 0.8816\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/100\n",
      "22500/22500 [==============================] - 12s 541us/step - loss: 0.1901 - acc: 0.9320 - val_loss: 0.2925 - val_acc: 0.8836\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.29359 to 0.29251, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 20/100\n",
      "22500/22500 [==============================] - 12s 534us/step - loss: 0.1852 - acc: 0.9343 - val_loss: 0.2924 - val_acc: 0.8836\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.29251 to 0.29240, saving model to models/imdb_bow.weights.h5\n",
      "Epoch 21/100\n",
      "22500/22500 [==============================] - 12s 547us/step - loss: 0.1805 - acc: 0.9360 - val_loss: 0.2932 - val_acc: 0.8828\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/100\n",
      "22500/22500 [==============================] - 12s 538us/step - loss: 0.1760 - acc: 0.9399 - val_loss: 0.2931 - val_acc: 0.8832\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/100\n",
      "22500/22500 [==============================] - 12s 528us/step - loss: 0.1715 - acc: 0.9415 - val_loss: 0.2935 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/100\n",
      "22500/22500 [==============================] - 13s 590us/step - loss: 0.1670 - acc: 0.9443 - val_loss: 0.2950 - val_acc: 0.8836\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/100\n",
      "22500/22500 [==============================] - 28s 1ms/step - loss: 0.1630 - acc: 0.9460 - val_loss: 0.2979 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/100\n",
      "22500/22500 [==============================] - 19s 832us/step - loss: 0.1593 - acc: 0.9470 - val_loss: 0.2968 - val_acc: 0.8820\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/100\n",
      "22500/22500 [==============================] - 12s 540us/step - loss: 0.1557 - acc: 0.9484 - val_loss: 0.2969 - val_acc: 0.8824\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/100\n",
      "22500/22500 [==============================] - 12s 546us/step - loss: 0.1518 - acc: 0.9503 - val_loss: 0.2982 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/100\n",
      "22500/22500 [==============================] - 12s 549us/step - loss: 0.1482 - acc: 0.9524 - val_loss: 0.2989 - val_acc: 0.8788\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/100\n",
      "22500/22500 [==============================] - 12s 533us/step - loss: 0.1448 - acc: 0.9528 - val_loss: 0.3004 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/100\n",
      "22500/22500 [==============================] - 12s 540us/step - loss: 0.1410 - acc: 0.9551 - val_loss: 0.3014 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/100\n",
      "22500/22500 [==============================] - 12s 553us/step - loss: 0.1380 - acc: 0.9568 - val_loss: 0.3054 - val_acc: 0.8824\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/100\n",
      "22500/22500 [==============================] - 12s 552us/step - loss: 0.1347 - acc: 0.9580 - val_loss: 0.3044 - val_acc: 0.8772\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/100\n",
      "22500/22500 [==============================] - 13s 560us/step - loss: 0.1319 - acc: 0.9590 - val_loss: 0.3078 - val_acc: 0.8828\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/100\n",
      "22500/22500 [==============================] - 12s 548us/step - loss: 0.1287 - acc: 0.9606 - val_loss: 0.3088 - val_acc: 0.8796\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/100\n",
      "22500/22500 [==============================] - 13s 595us/step - loss: 0.1261 - acc: 0.9617 - val_loss: 0.3083 - val_acc: 0.8796\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/100\n",
      "22500/22500 [==============================] - 12s 555us/step - loss: 0.1230 - acc: 0.9630 - val_loss: 0.3093 - val_acc: 0.8804\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/100\n",
      "22500/22500 [==============================] - 12s 554us/step - loss: 0.1206 - acc: 0.9645 - val_loss: 0.3119 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/100\n",
      "22500/22500 [==============================] - 13s 564us/step - loss: 0.1178 - acc: 0.9655 - val_loss: 0.3125 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500/22500 [==============================] - 12s 555us/step - loss: 0.1152 - acc: 0.9663 - val_loss: 0.3142 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/100\n",
      "22500/22500 [==============================] - 12s 550us/step - loss: 0.1127 - acc: 0.9681 - val_loss: 0.3155 - val_acc: 0.8796\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/100\n",
      "22500/22500 [==============================] - 13s 557us/step - loss: 0.1102 - acc: 0.9688 - val_loss: 0.3173 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/100\n",
      "22500/22500 [==============================] - 13s 559us/step - loss: 0.1079 - acc: 0.9704 - val_loss: 0.3195 - val_acc: 0.8788\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/100\n",
      "22500/22500 [==============================] - 12s 551us/step - loss: 0.1055 - acc: 0.9712 - val_loss: 0.3227 - val_acc: 0.8776\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/100\n",
      "22500/22500 [==============================] - 12s 553us/step - loss: 0.1036 - acc: 0.9718 - val_loss: 0.3234 - val_acc: 0.8776\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/100\n",
      "22500/22500 [==============================] - 12s 551us/step - loss: 0.1013 - acc: 0.9723 - val_loss: 0.3278 - val_acc: 0.8772\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/100\n",
      "22500/22500 [==============================] - 12s 552us/step - loss: 0.0992 - acc: 0.9736 - val_loss: 0.3269 - val_acc: 0.8768\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/100\n",
      "22500/22500 [==============================] - 12s 543us/step - loss: 0.0971 - acc: 0.9745 - val_loss: 0.3285 - val_acc: 0.8788\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/100\n",
      "22500/22500 [==============================] - 12s 550us/step - loss: 0.0951 - acc: 0.9754 - val_loss: 0.3303 - val_acc: 0.8764\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/100\n",
      "22500/22500 [==============================] - 12s 550us/step - loss: 0.0931 - acc: 0.9756 - val_loss: 0.3320 - val_acc: 0.8768\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/100\n",
      "22500/22500 [==============================] - 12s 538us/step - loss: 0.0910 - acc: 0.9768 - val_loss: 0.3344 - val_acc: 0.8772\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/100\n",
      "22500/22500 [==============================] - 12s 553us/step - loss: 0.0892 - acc: 0.9773 - val_loss: 0.3356 - val_acc: 0.8748\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/100\n",
      "22500/22500 [==============================] - 12s 544us/step - loss: 0.0872 - acc: 0.9789 - val_loss: 0.3379 - val_acc: 0.8764\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/100\n",
      "22500/22500 [==============================] - 12s 544us/step - loss: 0.0856 - acc: 0.9792 - val_loss: 0.3398 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/100\n",
      "22500/22500 [==============================] - 12s 547us/step - loss: 0.0838 - acc: 0.9794 - val_loss: 0.3416 - val_acc: 0.8748\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/100\n",
      "22500/22500 [==============================] - 12s 543us/step - loss: 0.0821 - acc: 0.9802 - val_loss: 0.3435 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/100\n",
      "22500/22500 [==============================] - 12s 536us/step - loss: 0.0805 - acc: 0.9810 - val_loss: 0.3451 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/100\n",
      "22500/22500 [==============================] - 12s 547us/step - loss: 0.0788 - acc: 0.9817 - val_loss: 0.3467 - val_acc: 0.8756\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/100\n",
      "22500/22500 [==============================] - 12s 540us/step - loss: 0.0774 - acc: 0.9825 - val_loss: 0.3520 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/100\n",
      "22500/22500 [==============================] - 12s 539us/step - loss: 0.0761 - acc: 0.9828 - val_loss: 0.3502 - val_acc: 0.8748\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/100\n",
      "22500/22500 [==============================] - 12s 526us/step - loss: 0.0744 - acc: 0.9836 - val_loss: 0.3529 - val_acc: 0.8772\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/100\n",
      "22500/22500 [==============================] - 12s 541us/step - loss: 0.0728 - acc: 0.9845 - val_loss: 0.3539 - val_acc: 0.8752\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/100\n",
      "22500/22500 [==============================] - 12s 545us/step - loss: 0.0715 - acc: 0.9844 - val_loss: 0.3569 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/100\n",
      "22500/22500 [==============================] - 12s 538us/step - loss: 0.0700 - acc: 0.9850 - val_loss: 0.3575 - val_acc: 0.8748\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/100\n",
      "22500/22500 [==============================] - 13s 564us/step - loss: 0.0687 - acc: 0.9857 - val_loss: 0.3594 - val_acc: 0.8768\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/100\n",
      "22500/22500 [==============================] - 12s 554us/step - loss: 0.0675 - acc: 0.9863 - val_loss: 0.3614 - val_acc: 0.8748\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/100\n",
      "22500/22500 [==============================] - 12s 553us/step - loss: 0.0659 - acc: 0.9870 - val_loss: 0.3629 - val_acc: 0.8752\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/100\n",
      "22500/22500 [==============================] - 21s 938us/step - loss: 0.0649 - acc: 0.9874 - val_loss: 0.3699 - val_acc: 0.8776\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/100\n",
      "22500/22500 [==============================] - 16s 689us/step - loss: 0.0635 - acc: 0.9880 - val_loss: 0.3666 - val_acc: 0.8756\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/100\n",
      "22500/22500 [==============================] - 12s 547us/step - loss: 0.0624 - acc: 0.9883 - val_loss: 0.3692 - val_acc: 0.8776\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/100\n",
      "22500/22500 [==============================] - 22s 960us/step - loss: 0.0612 - acc: 0.9884 - val_loss: 0.3703 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/100\n",
      "22500/22500 [==============================] - 23s 1ms/step - loss: 0.0600 - acc: 0.9886 - val_loss: 0.3723 - val_acc: 0.8764\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/100\n",
      "22500/22500 [==============================] - 21s 954us/step - loss: 0.0590 - acc: 0.9893 - val_loss: 0.3737 - val_acc: 0.8748\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/100\n",
      "22500/22500 [==============================] - 15s 655us/step - loss: 0.0579 - acc: 0.9894 - val_loss: 0.3762 - val_acc: 0.8752\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/100\n",
      "22500/22500 [==============================] - 18s 815us/step - loss: 0.0569 - acc: 0.9896 - val_loss: 0.3778 - val_acc: 0.8740\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/100\n",
      "22500/22500 [==============================] - 13s 557us/step - loss: 0.0560 - acc: 0.9904 - val_loss: 0.3802 - val_acc: 0.8732\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 77/100\n",
      "22500/22500 [==============================] - 24s 1ms/step - loss: 0.0548 - acc: 0.9906 - val_loss: 0.3810 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/100\n",
      "22500/22500 [==============================] - 13s 564us/step - loss: 0.0538 - acc: 0.9910 - val_loss: 0.3831 - val_acc: 0.8732\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 79/100\n",
      "22500/22500 [==============================] - 12s 527us/step - loss: 0.0530 - acc: 0.9908 - val_loss: 0.3856 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 80/100\n",
      "22500/22500 [==============================] - 18s 780us/step - loss: 0.0519 - acc: 0.9917 - val_loss: 0.3864 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/100\n",
      "22500/22500 [==============================] - 23s 1ms/step - loss: 0.0510 - acc: 0.9919 - val_loss: 0.3882 - val_acc: 0.8752\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 82/100\n",
      "22500/22500 [==============================] - 31s 1ms/step - loss: 0.0499 - acc: 0.9920 - val_loss: 0.3902 - val_acc: 0.8732\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/100\n",
      "22500/22500 [==============================] - 24s 1ms/step - loss: 0.0492 - acc: 0.9922 - val_loss: 0.3917 - val_acc: 0.8740\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 84/100\n",
      "22500/22500 [==============================] - 16s 690us/step - loss: 0.0484 - acc: 0.9924 - val_loss: 0.3930 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/100\n",
      "22500/22500 [==============================] - 29s 1ms/step - loss: 0.0475 - acc: 0.9926 - val_loss: 0.3960 - val_acc: 0.8768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/100\n",
      "22500/22500 [==============================] - 26s 1ms/step - loss: 0.0468 - acc: 0.9933 - val_loss: 0.3970 - val_acc: 0.8740\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/100\n",
      "22500/22500 [==============================] - 23s 1ms/step - loss: 0.0458 - acc: 0.9935 - val_loss: 0.3984 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 88/100\n",
      "22500/22500 [==============================] - 27s 1ms/step - loss: 0.0451 - acc: 0.9936 - val_loss: 0.4000 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 89/100\n",
      "22500/22500 [==============================] - 32s 1ms/step - loss: 0.0443 - acc: 0.9934 - val_loss: 0.4019 - val_acc: 0.8732\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 90/100\n",
      "22500/22500 [==============================] - 13s 594us/step - loss: 0.0436 - acc: 0.9940 - val_loss: 0.4033 - val_acc: 0.8740\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/100\n",
      "22500/22500 [==============================] - 12s 546us/step - loss: 0.0429 - acc: 0.9944 - val_loss: 0.4062 - val_acc: 0.8748\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 92/100\n",
      "22500/22500 [==============================] - 12s 541us/step - loss: 0.0421 - acc: 0.9944 - val_loss: 0.4073 - val_acc: 0.8728\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 93/100\n",
      "22500/22500 [==============================] - 12s 554us/step - loss: 0.0415 - acc: 0.9947 - val_loss: 0.4091 - val_acc: 0.8732\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 94/100\n",
      "22500/22500 [==============================] - 12s 554us/step - loss: 0.0407 - acc: 0.9948 - val_loss: 0.4104 - val_acc: 0.8728\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 95/100\n",
      "22500/22500 [==============================] - 12s 544us/step - loss: 0.0400 - acc: 0.9948 - val_loss: 0.4117 - val_acc: 0.8732\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/100\n",
      "22500/22500 [==============================] - 12s 552us/step - loss: 0.0394 - acc: 0.9950 - val_loss: 0.4131 - val_acc: 0.8732\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 97/100\n",
      "22500/22500 [==============================] - 12s 553us/step - loss: 0.0388 - acc: 0.9952 - val_loss: 0.4148 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 98/100\n",
      "22500/22500 [==============================] - 12s 540us/step - loss: 0.0382 - acc: 0.9955 - val_loss: 0.4166 - val_acc: 0.8732\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 99/100\n",
      "22500/22500 [==============================] - 12s 536us/step - loss: 0.0375 - acc: 0.9956 - val_loss: 0.4188 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      "Epoch 100/100\n",
      "22500/22500 [==============================] - 13s 560us/step - loss: 0.0370 - acc: 0.9958 - val_loss: 0.4199 - val_acc: 0.8748\n",
      "\n",
      "Epoch 00100: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def save_model(file_name,model,label_encoder,vectorizer):\n",
    "    \"\"\"Saves model structure and vocabularies\"\"\"\n",
    "    model_json = model.to_json()\n",
    "    with open(file_name+\".model.json\", \"w\") as f:\n",
    "        print(model_json,file=f)\n",
    "    with open(file_name+\".vocabularies.json\",\"w\") as f:\n",
    "        classes=list(label_encoder.classes_)\n",
    "        vocab=dict(((str(w),int(idx)) for w,idx in vectorizer.vocabulary_.items())) #must turn numpy objects to python ones\n",
    "        json.dump((classes,vocab),f,indent=2)\n",
    "        \n",
    "example_count,feature_count=feature_matrix.shape\n",
    "example_count2,class_count=classes_1hot.shape\n",
    "assert example_count==example_count2 #sanity check\n",
    "\n",
    "inp=Input(shape=(feature_count,))\n",
    "hidden=Dense(200,activation=\"tanh\")(inp)\n",
    "outp=Dense(class_count,activation=\"softmax\")(hidden)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "model.compile(optimizer=\"sgd\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "# Save model and vocabularies, can be done before training\n",
    "save_model(\"models/imdb_bow\",model,label_encoder,vectorizer)\n",
    "# Callback function to save weights during training, if validation loss goes down\n",
    "save_cb=ModelCheckpoint(filepath=\"models/imdb_bow.weights.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "hist=model.fit(feature_matrix,classes_1hot,batch_size=100,verbose=1,epochs=100,validation_split=0.1,callbacks=[save_cb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network output= [[1.8448678e-05 9.9998152e-01]\n",
      " [9.9508435e-01 4.9155969e-03]\n",
      " [9.6974520e-07 9.9999905e-01]\n",
      " ...\n",
      " [2.8421250e-01 7.1578753e-01]\n",
      " [1.3249072e-01 8.6750925e-01]\n",
      " [2.1320222e-06 9.9999785e-01]]\n",
      "Maximum class for each example= [1 0 1 ... 1 1 1]\n",
      "Confusion matrix=\n",
      " [[1101  157]\n",
      " [ 156 1086]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.88      0.88      0.88      1258\n",
      "        pos       0.87      0.87      0.87      1242\n",
      "\n",
      "avg / total       0.87      0.87      0.87      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#Validation data used during training:\n",
    "val_instances,val_labels_1hot,_=hist.validation_data\n",
    "\n",
    "print(\"Network output=\",model.predict(val_instances))\n",
    "predictions=numpy.argmax(model.predict(val_instances),axis=1)\n",
    "print(\"Maximum class for each example=\",predictions)\n",
    "gold=numpy.nonzero(val_labels_1hot)[1] #undo 1-hot encoding\n",
    "conf_matrix=confusion_matrix(list(gold),list(predictions))\n",
    "print(\"Confusion matrix=\\n\",conf_matrix)\n",
    "print(classification_report(list(gold),list(predictions),target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning progress\n",
    "\n",
    "* The history object we get lets us inspect the accuracy during training\n",
    "* Remarks:\n",
    "  - Accuracy on training data keeps going up\n",
    "  - Accuracy on validation (test) data flattens out after a but over 10 epochs, we are learning very little past that point\n",
    "  - What we see is the network keeps overfitting on the training data to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd582732390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#print(hist.history[\"val_acc\"])\n",
    "plt.ylim(0.85,1.0)\n",
    "plt.plot(hist.history[\"val_acc\"],label=\"Validation set accuracy\")\n",
    "plt.plot(hist.history[\"acc\"],label=\"Training set accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "* We put together a program to train a neural network classifier for sentiment detector\n",
    "* We learned the necessary code/techniques to save models, and feed the training with data in just the right format\n",
    "* We observed the training across epochs\n",
    "* We saw how the classifier can be applied to various text classification problems\n",
    "* The IMDB sentiment classifier ended up at nearly 90% accuracy, the state of the art is about 95%, we got surprisingly far in few lines of code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-jupyter",
   "language": "python",
   "name": "venv-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
