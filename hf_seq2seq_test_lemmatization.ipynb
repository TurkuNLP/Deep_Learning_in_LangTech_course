{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696421a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hf_seq2seq_lemmatization.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1M0LODo6g2mREdKOXunrg4mvjJUiSJ5tr\n",
    "\n",
    "# The task\n",
    "\n",
    "* Lemmatization\n",
    "* Input: wordform + morpho information\n",
    "* Output: word baseform\n",
    "* Easy for English, but not so much for Finnish or many other languages\n",
    "\n",
    "Here is few examples:\n",
    "\n",
    "* dogs+NOUN+Plural -> dog\n",
    "* sheep+NOUN+Plural -> sheep\n",
    "* voi+VERB+... -> voida\n",
    "* voi+NOUN+Singular -> voi\n",
    "\n",
    "# Data preparation\n",
    "\n",
    "* We can use universaldependencies.org\n",
    "* Collection of treebanks\n",
    "* Pick your favorite language, I will use Finnish\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cca60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --quiet datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a941f6",
   "metadata": {},
   "source": [
    "You can use UD_English-EWT for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb07ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O train.conllu https://github.com/UniversalDependencies/UD_Finnish-TDT/raw/master/fi_tdt-ud-train.conllu\n",
    "!wget -O validation.conllu https://github.com/UniversalDependencies/UD_Finnish-TDT/raw/master/fi_tdt-ud-dev.conllu\n",
    "!wget -O test.conllu https://github.com/UniversalDependencies/UD_Finnish-TDT/raw/master/fi_tdt-ud-test.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ab6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Data preparation\n",
    "\n",
    "* The CoNLL format should be familiar to you by now\n",
    "* Here is few lines (the delimiter is TAB)\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# newdoc id = weblog-blogspot.com_zentelligence_20040423000200_ENG_20040423_000200\n",
    "# sent_id = weblog-blogspot.com_zentelligence_20040423000200_ENG_20040423_000200-0001\n",
    "# newpar id = weblog-blogspot.com_zentelligence_20040423000200_ENG_20040423_000200-p0001\n",
    "# text = What if Google Morphed Into GoogleOS?\n",
    "1\tWhat\twhat\tPRON\tWP\tPronType=Int\t0\troot\t0:root\t_\n",
    "2\tif\tif\tSCONJ\tIN\t_\t4\tmark\t4:mark\t_\n",
    "3\tGoogle\tGoogle\tPROPN\tNNP\tNumber=Sing\t4\tnsubj\t4:nsubj\t_\n",
    "4\tMorphed\tmorph\tVERB\tVBD\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\t1\tadvcl\t1:advcl:if\t_\n",
    "5\tInto\tinto\tADP\tIN\t_\t6\tcase\t6:case\t_\n",
    "6\tGoogleOS\tGoogleOS\tPROPN\tNNP\tNumber=Sing\t4\tobl\t4:obl:into\tSpaceAfter=No\n",
    "7\t?\t?\tPUNCT\t.\t_\t4\tpunct\t4:punct\t_\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "* Let us form training examples like so:\n",
    "    * Input is `wordform`_`POS`_`FEATS`\n",
    "    * Output is the lemma\n",
    "* We can reuse part of our dataset preparation code from the [MLP notebook](https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/hf_trainer_mlp.ipynb)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cbccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef8dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "    'json',                             # Format of the data\n",
    "    data_files={\"test\":\"test.jsonl\"},\n",
    "    split={\n",
    "        \"test\":\"test\"\n",
    "    },\n",
    "    features=datasets.Features({    # Here we tell how to interpret the attributes\n",
    "        \"form_tags\":datasets.Value(\"string\"),\n",
    "        \"lemma\":datasets.Value(\"string\")\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8a707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.shuffle()\n",
    "#that was easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6702c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe89ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Tokenize and prep\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4982a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a8d8ac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#OK, let's try with our trusty tokenizer\n",
    "#but why would this work in the first place?\n",
    "model_name = \"TurkuNLP/bert-base-finnish-cased-v1\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({ \"additional_special_tokens\": [ \"[unused1]\", \"[unused2]\" ] })\n",
    "\"\"\"the examples are formed surprisingly similarly to what you've seen before:\n",
    "\n",
    "* `input_ids` is the input side\n",
    "* `attention_mask` is the input attention mask\n",
    "* `labels` is the output ids\n",
    "* the encoder-decoder model should take care of the rest\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d493d54",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    \n",
    "    inp_w,inp_tags=example[\"form_tags\"].split(\"+++\",1)\n",
    "    out=\" \".join(example[\"lemma\"])\n",
    "    \n",
    "    inp_tok=tokenizer(\"[unused1] \"+\" \".join(inp_w)+\" \"+(inp_tags.replace(\"|\",\" \"))+\" [unused2]\",truncation=True)\n",
    "    outp_tok=tokenizer(\"[unused1] \"+out+\" [unused2]\",truncation=True)\n",
    "\n",
    "    return {\"input_ids\":inp_tok[\"input_ids\"],\n",
    "            \"attention_mask\":inp_tok[\"attention_mask\"],\n",
    "            \"labels\":outp_tok[\"input_ids\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df4b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94855d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"s2s_lemmatizer\")\n",
    "#help(model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaea6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator=transformers.DataCollatorForSeq2Seq(tokenizer=tokenizer,\n",
    "                                             model=model,\n",
    "                                             padding=True,\n",
    "                                             return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9458682",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]\n",
    "for e in dataset[\"test\"]:\n",
    "    lst.append({\"input_ids\":e[\"input_ids\"],\"labels\":e[\"labels\"],\"attention_mask\":e[\"attention_mask\"]})\n",
    "    break\n",
    "batch=collator(lst)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7dfec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_args = transformers.Seq2SeqTrainingArguments(\n",
    "    \"checkpoints\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=128,\n",
    "    max_steps=30000,\n",
    "    save_steps=1000,\n",
    "    predict_with_generate=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c94b1",
   "metadata": {},
   "outputs": [],
   "source": [
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = transformers.EarlyStoppingCallback(\n",
    "    early_stopping_patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa24197",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=trainer_args,\n",
    "    data_collator=collator,\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5678904",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=dataset[\"test\"].select(range(133))\n",
    "predictions=trainer.predict(test_data)\n",
    "for x,e in zip(predictions.predictions,test_data):\n",
    "    print(e[\"form_tags\"],tokenizer.decode(x))\n",
    "#print(model(batch))\n",
    "#print(batch)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
